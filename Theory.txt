ğŸ“˜ LINEAR REGRESSION

ğŸ“ŒAssumptions in Linear Regression:
1)Linearity- The relationship between the independent (X) and dependent (Y) variables should be linear. Check using scatter plots or residual plots.
2) Homoscedasticity (Constant Variance of Error)- The variance of residuals should remain constant for all values of X. Tests: Breusch-Pagan Test, White Test, Plot: Residual vs Fitted values plot.
3) Normality of Error Terms- Residuals should be normally distributed. Test: Anderson-Darling Test, Shapiro-Wilk, Q-Q Plot.
4) Independence of Error Terms- Error terms should be independent across observations. Test: Durbin-Watson Test.
5) No Multicollinearity- Independent variables should not be highly correlated. Test: Variance Inflation Factor (VIF) (VIF > 5 or 10 â†’ multicollinearity present).
6) No Influential Outliers- Outliers that have a large impact on the model should be identified and handled.Method: Cookâ€™s Distance, Leverage score.

ğŸ“ŒCost Function (Loss Function):
1) Mean Square Error- Used to evaluate the performance of the model:{ h0(x)=0o+01x }
2) Hypothesis Function- Assumptions relation between X an Y
J(o)= (1/2m) sum{1 to m} (y_true - y_predicted)^2

ğŸ“ŒVisualise Cost fuction and Gradient Desecent:
1) The Cost function of linear regression is convex (parabolic shape OR U-shape). It ensures global minimum.
2) Gradient-Descent Starts at the random point and moves downhill to find the global minimum. It minimizes the cost function by iteratively updating parameters.

ğŸ“ŒTypes of Gradient Descent: 
1) Batch Gradient Descent â€“ Uses all data points each iteration (stable but slow).
2) Stochastic Gradient Descent (SGD) â€“ Uses one data point per iteration (faster but noisier).
3) Mini-batch Gradient Descent â€“ Uses a small batch of data (balance between speed & accuracy).

ğŸ“Œ Example:
X = [1, 2, 3]
Y = [2, 2.8, 3.6]
Initial: Î¸â‚€ = 0, Î¸â‚ = 0, m = 3

ğŸ“ŒTechnique of Feature Selection:
A) Manual Feature Selection- Based on domain knowledge and intuition.

B) Statistical Method- 
    1) Correlation anlysis P-value hypothesis testing (p-value > 0.05 for statistically insignificant).  
    2) ANOVA: For comparing means across groups.
    3) Pearson Correlation (numerical)
    4) Spearman & Kendall Correlation (ordinal)
    5) Variance Threshold: Removes low variance features.
        Pearson Cprrelation ( r= sum{ (x1-x_mean)(y1-y_mean) }/{ (sum(x1-x_mean)^2 . sum(y1-y_mean)^2)^0.5 })
        df.corr(method="pearson") - numerical
        df.corr(method="spearman") - ordinal values
        df.corr(method="kendall") - ordinal values
        Variance Threshhold- sum(x1-x)^2 / N

C) Automated Feature Selection Methods- 
    1) Recursive Feature Elimination (RFE)
    2) Lasso Regression (L1 Regularization) â€“ Shrinks less important coefficients to 0.
    3) Ridge Regression (L2 Regularization) â€“ Shrinks coefficients but doesnâ€™t remove them.
    4) Tree-Based Feature Importance â€“ Feature importances from decision trees/random forests.

ğŸ“ŒEvaluation Metrics for Regression
1) Mean Squared Error (MSE)
2) Mean Absolute Error (MAE)
3) R-squared (RÂ²) â€“ Proportion of variance explained by the model
4) Adjusted R-squared â€“ Adjusts RÂ² for number of predictors

ğŸ“ŒClassification Metrics:
Accuracy= (TP+TN)/ (TP+TN+FP+FN)
Precison= TP/(TP+FP)
Recall(sensitivity)= TP/(TP+FN)
F1 Score= 2* (Precison*Recall)/(Precison+Recall)

-------------------------------------------------------------------------------------------------------------------------------

ğŸ“˜ LOGISTIC REGRESSION

Logistic Regression uses Sigmoid Function to map predictions to a probability between 0 and 1:
                    ğœ(ğ‘§)=1 / 1+ğ‘’âˆ’z


ğŸ“ŒTypes:
1) Binomial: Two categories (e.g., Yes/No)
2) Multinomial: More than two unordered categories (e.g., Red/Blue/Green)
3) Ordinal: More than two ordered categories (e.g., Low/Medium/High)

ğŸ“ŒAssumptions:
1) Binary or categorical dependent variable
2) Independent observations
3) No multicollinearity
4) No extreme outliers
5) Large sample size

-------------------------------------------------------------------------------------------------------------------------------

ğŸ“˜ DECISION TREE

ğŸ“ŒKey Concepts:
1) Recursive Partitioning of data into subsets based on feature values.
2) Splitting Criteria: Gini Index (used in CART), Entropy / Information Gain (used in ID3)

ğŸ“ŒAdvantages:
1) Easy to interpret
2) Handles both numerical and categorical data
3) No need for feature scaling

ğŸ“ŒDisadvantages:
1) Prone to overfitting (solved by pruning)
2) Can be biased with imbalanced datasets

bbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbb